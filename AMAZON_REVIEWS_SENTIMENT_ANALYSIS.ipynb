{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alomo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alomo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\alomo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"words\")\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING:REMOVING STOPWORDS,NON-ENGLISH WORDS AND LEMMATIZING AND FILTERING WORDS WITH LENGTH LESS THAN 3.\n",
    "def remove_underscores(sentence):\n",
    "    sentence= sentence.replace(\"_\",\" \")\n",
    "    return sentence\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_extra_words(sentence):     #remove stop words and meaningless words\n",
    "    new_sentence=\"\"\n",
    "    for w in sentence.split():\n",
    "        w=w.lower()\n",
    "        if w in english_words and w not in stop_words and w.isalpha():\n",
    "            new_sentence=new_sentence+\" \"+w\n",
    "    return new_sentence\n",
    "\n",
    "def lemmatize_and_filter(sentence, min_word_length):        #to lemmatize words and lose the ones with length less than equal to 3.\n",
    "    sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        word=word.lower()\n",
    "        if len(lemmatizer.lemmatize(word))>3:\n",
    "            sent= sent+\" \"+lemmatizer.lemmatize(word)\n",
    "    return(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_label_review(data):\n",
    "    #takes in list of raw data and returns list of string sentences and the list of their corresponding labels.\n",
    "    labels = [int(re.findall(\"1|2\", str(lines))[0]) for lines in data]\n",
    "    reviews = [re.split(\"__label__[1|2]\",str(lines))[1].strip().lower() for lines in data]\n",
    "    return(reviews,labels)\n",
    "\n",
    "\n",
    "def preprocessed_data(data):        #takes in list of raw data and returns list of processed string sentences and the list of their corresponding labels.\n",
    "    reviews = [lemmatize_and_filter(remove_extra_words(BeautifulSoup(re.sub(r'[^\\w\\s]|^https?:\\/\\/.*[\\r\\n]*|\\d+', '', remove_underscores(str(lines)).strip().lower()), \"lxml\").text),3) for lines in data]\n",
    "    return(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  **TRAIN-VALIDATION SPLIT**\n",
    "\n",
    "def split_train_into_tain_validate(data_,validation_ratio):\n",
    "    \n",
    "    data=data_\n",
    "    \n",
    "    train_size = int(len(data)*(1-validation_ratio))\n",
    "    \n",
    "    random.shuffle(data)\n",
    "\n",
    "    validate_samples= data[train_size:]\n",
    "\n",
    "    data = data[:train_size]\n",
    "    \n",
    "    return(data,validate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **NEGATIVE POSITIVE SPLIT**\n",
    "#SPLITING THE DATA INTO POSITIVE AND NEGATIVE REVIEWS SEPARATELY.\n",
    "import re\n",
    "def split_into_negative_positive(data):\n",
    "    negative_reviews=[]      # list of all negative reviews\n",
    "    positive_reviews=[]      # list of all positive reviews\n",
    "\n",
    "    for lines in train_file_lines:\n",
    "        lines= str(lines).lower()\n",
    "        x=re.findall(\"1|2\", lines)[0]\n",
    "        if x==\"1\":\n",
    "            negative_reviews.append(lines)\n",
    "        elif x==\"2\":\n",
    "            positive_reviews.append(lines)\n",
    "    return(negative_reviews, positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews,positive_reviews= split_into_negative_positive(train_file_lines)\n",
    "\n",
    "negative_reviews,labels = split_label_review(negative_reviews)\n",
    "print(\"First few negative reviews:\",negative_reviews[:5])\n",
    "negative_reviews=preprocessed_data(negative_reviews)\n",
    "\n",
    "positive_reviews,labels = split_label_review(positive_reviews)\n",
    "print(\"First few positive reviews:\",positive_reviews[:5])\n",
    "positive_reviews=preprocessed_data(positive_reviews)\n",
    "\n",
    "del positive_reviews, negative_reviews,labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

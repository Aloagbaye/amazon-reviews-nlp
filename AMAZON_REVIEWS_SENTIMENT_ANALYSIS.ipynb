{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alomo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alomo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\alomo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"amazonreviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>Product Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Suits more than all my needs. I made all the...</td>\n",
       "      <td>/VivoBook-Touchscreen-Processor-Fingerprint-J4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I got this as a second laptop, mainly to tak...</td>\n",
       "      <td>/VivoBook-Touchscreen-Processor-Fingerprint-J4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>So right away there's a problem......It cant...</td>\n",
       "      <td>/VivoBook-Touchscreen-Processor-Fingerprint-J4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>So far this computer has been working as des...</td>\n",
       "      <td>/VivoBook-Touchscreen-Processor-Fingerprint-J4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>It came with the keyboard disabled. I soent ...</td>\n",
       "      <td>/VivoBook-Touchscreen-Processor-Fingerprint-J4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            reviews  \\\n",
       "0           1    Suits more than all my needs. I made all the...   \n",
       "1           2    I got this as a second laptop, mainly to tak...   \n",
       "2           3    So right away there's a problem......It cant...   \n",
       "3           4    So far this computer has been working as des...   \n",
       "4           5    It came with the keyboard disabled. I soent ...   \n",
       "\n",
       "                                       Product Links  \n",
       "0  /VivoBook-Touchscreen-Processor-Fingerprint-J4...  \n",
       "1  /VivoBook-Touchscreen-Processor-Fingerprint-J4...  \n",
       "2  /VivoBook-Touchscreen-Processor-Fingerprint-J4...  \n",
       "3  /VivoBook-Touchscreen-Processor-Fingerprint-J4...  \n",
       "4  /VivoBook-Touchscreen-Processor-Fingerprint-J4...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular expression tokenizer\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "txt_1 = df['reviews'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Suits more than all my needs. I made all the updates yesterday and everything worked fine and today, Google offers me to fulfil the form for the warranty. This notebook flip is very amazing and it is nearly zero noise. It goes up to 2.8Ghz speed, connect to the 5G, battery last all day and it took me only 3 hours for a full charge. It also provides a lot of good apps and tools from Microsoft and Google. So, like I was only expecting a typewriter for my writing work, this one is a real gift for the price. Large screen, metal solid device (a bit heavy compare to the new ones), smooth keys to type, touchscreen works fine and funny flip to use too. To make a long story short, I love it! But in my case, unfortunately for my work, I need more memory.  So I decided to returning it and buy another one.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(txt_1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suits',\n",
       " 'more',\n",
       " 'than',\n",
       " 'all',\n",
       " 'my',\n",
       " 'needs',\n",
       " 'i',\n",
       " 'made',\n",
       " 'all',\n",
       " 'the',\n",
       " 'updates',\n",
       " 'yesterday',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'worked',\n",
       " 'fine',\n",
       " 'and',\n",
       " 'today',\n",
       " 'google',\n",
       " 'offers',\n",
       " 'me',\n",
       " 'to',\n",
       " 'fulfil',\n",
       " 'the',\n",
       " 'form',\n",
       " 'for',\n",
       " 'the',\n",
       " 'warranty',\n",
       " 'this',\n",
       " 'notebook',\n",
       " 'flip',\n",
       " 'is',\n",
       " 'very',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nearly',\n",
       " 'zero',\n",
       " 'noise',\n",
       " 'it',\n",
       " 'goes',\n",
       " 'up',\n",
       " 'to',\n",
       " '2',\n",
       " '8ghz',\n",
       " 'speed',\n",
       " 'connect',\n",
       " 'to',\n",
       " 'the',\n",
       " '5g',\n",
       " 'battery',\n",
       " 'last',\n",
       " 'all',\n",
       " 'day',\n",
       " 'and',\n",
       " 'it',\n",
       " 'took',\n",
       " 'me',\n",
       " 'only',\n",
       " '3',\n",
       " 'hours',\n",
       " 'for',\n",
       " 'a',\n",
       " 'full',\n",
       " 'charge',\n",
       " 'it',\n",
       " 'also',\n",
       " 'provides',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'good',\n",
       " 'apps',\n",
       " 'and',\n",
       " 'tools',\n",
       " 'from',\n",
       " 'microsoft',\n",
       " 'and',\n",
       " 'google',\n",
       " 'so',\n",
       " 'like',\n",
       " 'i',\n",
       " 'was',\n",
       " 'only',\n",
       " 'expecting',\n",
       " 'a',\n",
       " 'typewriter',\n",
       " 'for',\n",
       " 'my',\n",
       " 'writing',\n",
       " 'work',\n",
       " 'this',\n",
       " 'one',\n",
       " 'is',\n",
       " 'a',\n",
       " 'real',\n",
       " 'gift',\n",
       " 'for',\n",
       " 'the',\n",
       " 'price',\n",
       " 'large',\n",
       " 'screen',\n",
       " 'metal',\n",
       " 'solid',\n",
       " 'device',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'heavy',\n",
       " 'compare',\n",
       " 'to',\n",
       " 'the',\n",
       " 'new',\n",
       " 'ones',\n",
       " 'smooth',\n",
       " 'keys',\n",
       " 'to',\n",
       " 'type',\n",
       " 'touchscreen',\n",
       " 'works',\n",
       " 'fine',\n",
       " 'and',\n",
       " 'funny',\n",
       " 'flip',\n",
       " 'to',\n",
       " 'use',\n",
       " 'too',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'long',\n",
       " 'story',\n",
       " 'short',\n",
       " 'i',\n",
       " 'love',\n",
       " 'it',\n",
       " 'but',\n",
       " 'in',\n",
       " 'my',\n",
       " 'case',\n",
       " 'unfortunately',\n",
       " 'for',\n",
       " 'my',\n",
       " 'work',\n",
       " 'i',\n",
       " 'need',\n",
       " 'more',\n",
       " 'memory',\n",
       " 'so',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'returning',\n",
       " 'it',\n",
       " 'and',\n",
       " 'buy',\n",
       " 'another',\n",
       " 'one']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "nltk_stpwd = stopwords.words('english')\n",
    "print(len(set(nltk_stpwd)))\n",
    "print(nltk_stpwd[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['suits', 'needs', 'made', 'updates', 'yesterday', 'everything', 'worked', 'fine', 'today', 'google']\n"
     ]
    }
   ],
   "source": [
    "stopped_tokens = [token for token in tokens if not token in nltk_stpwd]\n",
    "print(stopped_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews=df['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_stemmer = nltk.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['got', 'second', 'laptop', 'main', 'take', 'note', '2', '1', 'figur', 'would', 'support', 'pen', 'input', 'first', 'thing', 'first', 'get', 'comput', 'charger', 'box', 'comput', 'metal', 'nice', 'plus', 'price', 'rang', 'howev', 'build', 'qualiti', 'leav', 'lot', 'desir', 'signific', 'keyboard', 'screen', 'flex', 'overal', 'keyboard', 'good', 'also', 'move', 'cap', 'lock', 'indic', 'actual', 'cap', 'lock', 'key', 'chassi', 'trackpad', 'decent', 'size', 'huge', 'fan', 'dedic', 'left', 'right', 'click', 'button', 'know', 'peopl', 'prefer', 'window', 'precis', 'trackpad', 'support', 'window', 'gestur', 'fingerprint', 'scanner', 'recogn', 'finger', 'matter', 'mani', 'time', 'tri', 'term', 'port', 'great', 'microusb', 'port', 'serv', 'usb', '2', '0', 'port', 'micro', 'hdmi', 'port', 'headphon', 'jack', 'one', 'usb', 'c', 'port', 'one', 'sd', 'card', 'reader', 'barrel', 'charg', 'plug', 'understand', 'reason', 'ad', 'microusb', 'microhdmi', 'go', 'use', 'usb', 'c', 'port', 'anyway', 'could', 'ad', 'anoth', 'usb', 'c', 'talk', 'perform', 'let', 'talk', 'screen', '1366x768', 'realiz', 'forgot', 'non', 'full', 'hd', 'screen', 'look', 'like', 'count', 'pixel', 'huge', 'deal', 'laptop', 'mode', 'take', 'note', 'becom', 'obvious', 'end', 'want', 'take', 'note', 'speak', 'take', 'note', 'tablet', 'mode', 'dell', 'pen', 'work', 'splendid', 'laptop', 'definit', 'support', 'pen', 'input', 'anoth', 'problem', 'tablet', 'mode', 'view', 'angl', 'tn', 'panel', 'mean', 'horribl', 'view', 'angl', 'borderlin', 'imposs', 'see', 'type', 'unless', 'look', 'laptop', 'straight', 'let', 'move', 'onto', 'perform', '2', 'core', '2', 'thread', 'intel', 'celeron', '4', 'gigabyt', 'ram', 'combin', '128gb', 'fast', 'storag', 'actual', 'half', 'bad', 'daili', 'task', 'like', '2', 'tab', 'open', 'chrome', 'anyth', 'start', 'slow', 'freez', '4', 'gigabyt', 'ram', 'enough', 'wors', 'upgrad', 'goe', 'storag', 'solder', 'even', 'bother', 'run', 'cinebench', 'pcmark10', 'knew', 'go', 'perform', 'well', 'would', 'still', 'okay', 'keep', 'essenti', 'get', 'comput', 'price', 'ipad', 'horribl', 'batteri', 'life', 'would', 'last', 'hour', 'half', 'updat', 'done', 'matter', 'power', 'set', 'ultim', 'return', 'mine', 'sure', 'batteri', 'problem', 'otherwis', 'could', 'make', 'passabl', 'entri', 'level', 'comput']\n"
     ]
    }
   ],
   "source": [
    "num_reviews = df_reviews.shape[0]\n",
    "doc_set = [df_reviews[i] for i in range(num_reviews)]\n",
    "texts = []\n",
    "for doc in doc_set:\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    stopped_tokens = [token for token in tokens if not token in nltk_stpwd]\n",
    "    stemmed_tokens = [sb_stemmer.stem(token) for token in stopped_tokens]\n",
    "    texts.append(stemmed_tokens)# Adds tokens to new list \"texts\"\n",
    "    \n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_dict = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(7680 unique tokens: ['2', '3', '5g', '8ghz', 'also']...)\n"
     ]
    }
   ],
   "source": [
    "texts_dict.save('reviews.dict') \n",
    "print(texts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs 1 through 10: [('2', 0), ('3', 1), ('5g', 2), ('8ghz', 3), ('also', 4), ('amaz', 5), ('anoth', 6), ('app', 7), ('batteri', 8), ('bit', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(\"IDs 1 through 10: {}\".format(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordcheck = df_reviews.str.contains(\"price\").value_counts()\n",
    "ax = wordcheck.plot.bar(rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3', 0), ('amaz', 1), ('anoth', 2), ('app', 3), ('bit', 4), ('case', 5), ('charg', 6), ('compar', 7), ('connect', 8), ('day', 9)]\n"
     ]
    }
   ],
   "source": [
    "texts_dict.filter_extremes(no_below=20, no_above=0.10) \n",
    "print(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4891"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [texts_dict.doc2bow(text) for text in texts]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a corpus to disk in the sparse coordinate Matrix Market format in a serialized format instead of random\n",
    "corpora.MmCorpus.serialize('amzn_review.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Fit LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus,alpha='auto', num_topics=5,id2word=texts_dict, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x26086b7e1d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.059*\"de\" + 0.046*\"la\" + 0.033*\"que\" + 0.033*\"un\" + 0.024*\"en\"'),\n",
       " (1,\n",
       "  '0.010*\"app\" + 0.010*\"chromebook\" + 0.008*\"updat\" + 0.008*\"day\" + 0.008*\"problem\"'),\n",
       " (2,\n",
       "  '0.009*\"qualiti\" + 0.008*\"perform\" + 0.008*\"fan\" + 0.008*\"set\" + 0.008*\"video\"'),\n",
       " (3, '0.022*\"usb\" + 0.018*\"port\" + 0.010*\"touch\" + 0.010*\"c\" + 0.009*\"3\"'),\n",
       " (4,\n",
       "  '0.018*\"support\" + 0.017*\"amazon\" + 0.016*\"day\" + 0.015*\"acer\" + 0.014*\"month\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(num_topics=5,num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_query = df_reviews[0]\n",
    "query_words = raw_query.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Suits',\n",
       " 'more',\n",
       " 'than',\n",
       " 'all',\n",
       " 'my',\n",
       " 'needs.',\n",
       " 'I',\n",
       " 'made',\n",
       " 'all',\n",
       " 'the',\n",
       " 'updates',\n",
       " 'yesterday',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'worked',\n",
       " 'fine',\n",
       " 'and',\n",
       " 'today,',\n",
       " 'Google',\n",
       " 'offers',\n",
       " 'me',\n",
       " 'to',\n",
       " 'fulfil',\n",
       " 'the',\n",
       " 'form',\n",
       " 'for',\n",
       " 'the',\n",
       " 'warranty.',\n",
       " 'This',\n",
       " 'notebook',\n",
       " 'flip',\n",
       " 'is',\n",
       " 'very',\n",
       " 'amazing',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nearly',\n",
       " 'zero',\n",
       " 'noise.',\n",
       " 'It',\n",
       " 'goes',\n",
       " 'up',\n",
       " 'to',\n",
       " '2.8Ghz',\n",
       " 'speed,',\n",
       " 'connect',\n",
       " 'to',\n",
       " 'the',\n",
       " '5G,',\n",
       " 'battery',\n",
       " 'last',\n",
       " 'all',\n",
       " 'day',\n",
       " 'and',\n",
       " 'it',\n",
       " 'took',\n",
       " 'me',\n",
       " 'only',\n",
       " '3',\n",
       " 'hours',\n",
       " 'for',\n",
       " 'a',\n",
       " 'full',\n",
       " 'charge.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'provides',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'good',\n",
       " 'apps',\n",
       " 'and',\n",
       " 'tools',\n",
       " 'from',\n",
       " 'Microsoft',\n",
       " 'and',\n",
       " 'Google.',\n",
       " 'So,',\n",
       " 'like',\n",
       " 'I',\n",
       " 'was',\n",
       " 'only',\n",
       " 'expecting',\n",
       " 'a',\n",
       " 'typewriter',\n",
       " 'for',\n",
       " 'my',\n",
       " 'writing',\n",
       " 'work,',\n",
       " 'this',\n",
       " 'one',\n",
       " 'is',\n",
       " 'a',\n",
       " 'real',\n",
       " 'gift',\n",
       " 'for',\n",
       " 'the',\n",
       " 'price.',\n",
       " 'Large',\n",
       " 'screen,',\n",
       " 'metal',\n",
       " 'solid',\n",
       " 'device',\n",
       " '(a',\n",
       " 'bit',\n",
       " 'heavy',\n",
       " 'compare',\n",
       " 'to',\n",
       " 'the',\n",
       " 'new',\n",
       " 'ones),',\n",
       " 'smooth',\n",
       " 'keys',\n",
       " 'to',\n",
       " 'type,',\n",
       " 'touchscreen',\n",
       " 'works',\n",
       " 'fine',\n",
       " 'and',\n",
       " 'funny',\n",
       " 'flip',\n",
       " 'to',\n",
       " 'use',\n",
       " 'too.',\n",
       " 'To',\n",
       " 'make',\n",
       " 'a',\n",
       " 'long',\n",
       " 'story',\n",
       " 'short,',\n",
       " 'I',\n",
       " 'love',\n",
       " 'it!',\n",
       " 'But',\n",
       " 'in',\n",
       " 'my',\n",
       " 'case,',\n",
       " 'unfortunately',\n",
       " 'for',\n",
       " 'my',\n",
       " 'work,',\n",
       " 'I',\n",
       " 'need',\n",
       " 'more',\n",
       " 'memory.',\n",
       " 'So',\n",
       " 'I',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'returning',\n",
       " 'it',\n",
       " 'and',\n",
       " 'buy',\n",
       " 'another',\n",
       " 'one.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-71ac47a85e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mq_stopped_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mq_tokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk_stpwd\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mq_stemmed_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msb_stemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mq_stopped_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_stemmed_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "query = []\n",
    "for word in df_reviews[0]:\n",
    "    q_tokens = tokenizer.tokenize(word.lower())\n",
    "    q_stopped_tokens = [word for word in q_tokens if not word in nltk_stpwd]\n",
    "    q_stemmed_tokens = [sb_stemmer.stem(word) for word in q_stopped_tokens]\n",
    "    query.append(q_stemmed_tokens[0])\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-8bba010c1528>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msorted_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#Assessing least related topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#least related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#Assessing most related topics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\__init__.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, bow)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mold2new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moldid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moldid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbow\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moldid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mold2new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Words in query will be converted to ids and frequencies  \n",
    "id2word = gensim.corpora.Dictionary()\n",
    "_= id2word.merge_with(texts_dict) # garbage\n",
    "# Convert this document into (word, frequency) pairs\n",
    "query = id2word.doc2bow(query)\n",
    "print(query)\n",
    "#Create a sorted list\n",
    "sorted_list = list(sorted(lda_model[query], key=lambda x: x[1]))\n",
    "sorted_list\n",
    "#Assessing least related topics\n",
    "lda_model.print_topic(a[0][0]) #least related\n",
    "#Assessing most related topics\n",
    "lda_model.print_topic(a[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['laptop']\n"
     ]
    }
   ],
   "source": [
    "raw_query = 'laptops'\n",
    "\n",
    "query_words = raw_query.split()\n",
    "query = []\n",
    "for word in query_words:\n",
    "    # ad-hoc reuse steps from above\n",
    "    q_tokens = tokenizer.tokenize(word.lower())\n",
    "    q_stopped_tokens = [word for word in q_tokens if not word in nltk_stpwd]\n",
    "    q_stemmed_tokens = [sb_stemmer.stem(word) for word in q_stopped_tokens]\n",
    "    query.append(q_stemmed_tokens[0])\n",
    "    \n",
    "print(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel # To use the LDA model\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING:REMOVING STOPWORDS,NON-ENGLISH WORDS AND LEMMATIZING AND FILTERING WORDS WITH LENGTH LESS THAN 3.\n",
    "def remove_underscores(sentence):\n",
    "    sentence= sentence.replace(\"_\",\" \")\n",
    "    return sentence\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_extra_words(sentence):     #remove stop words and meaningless words\n",
    "    new_sentence=\"\"\n",
    "    for w in sentence.split():\n",
    "        w=w.lower()\n",
    "        if w in english_words and w not in stop_words and w.isalpha():\n",
    "            new_sentence=new_sentence+\" \"+w\n",
    "    return new_sentence\n",
    "\n",
    "def lemmatize_and_filter(sentence, min_word_length):        #to lemmatize words and lose the ones with length less than equal to 3.\n",
    "    sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        word=word.lower()\n",
    "        if len(lemmatizer.lemmatize(word))>3:\n",
    "            sent= sent+\" \"+lemmatizer.lemmatize(word)\n",
    "    return(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_label_review(data):\n",
    "    #takes in list of raw data and returns list of string sentences and the list of their corresponding labels.\n",
    "    labels = [int(re.findall(\"1|2\", str(lines))[0]) for lines in data]\n",
    "    reviews = [re.split(\"__label__[1|2]\",str(lines))[1].strip().lower() for lines in data]\n",
    "    return(reviews,labels)\n",
    "\n",
    "\n",
    "def preprocessed_data(data):        #takes in list of raw data and returns list of processed string sentences and the list of their corresponding labels.\n",
    "    reviews = [lemmatize_and_filter(remove_extra_words(BeautifulSoup(re.sub(r'[^\\w\\s]|^https?:\\/\\/.*[\\r\\n]*|\\d+', '', remove_underscores(str(lines)).strip().lower()), \"lxml\").text),3) for lines in data]\n",
    "    return(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  **TRAIN-VALIDATION SPLIT**\n",
    "\n",
    "def split_train_into_tain_validate(data_,validation_ratio):\n",
    "    \n",
    "    data=data_\n",
    "    \n",
    "    train_size = int(len(data)*(1-validation_ratio))\n",
    "    \n",
    "    random.shuffle(data)\n",
    "\n",
    "    validate_samples= data[train_size:]\n",
    "\n",
    "    data = data[:train_size]\n",
    "    \n",
    "    return(data,validate_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **NEGATIVE POSITIVE SPLIT**\n",
    "#SPLITING THE DATA INTO POSITIVE AND NEGATIVE REVIEWS SEPARATELY.\n",
    "import re\n",
    "def split_into_negative_positive(data):\n",
    "    negative_reviews=[]      # list of all negative reviews\n",
    "    positive_reviews=[]      # list of all positive reviews\n",
    "\n",
    "    for lines in train_file_lines:\n",
    "        lines= str(lines).lower()\n",
    "        x=re.findall(\"1|2\", lines)[0]\n",
    "        if x==\"1\":\n",
    "            negative_reviews.append(lines)\n",
    "        elif x==\"2\":\n",
    "            positive_reviews.append(lines)\n",
    "    return(negative_reviews, positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews,positive_reviews= split_into_negative_positive(train_file_lines)\n",
    "\n",
    "negative_reviews,labels = split_label_review(negative_reviews)\n",
    "print(\"First few negative reviews:\",negative_reviews[:5])\n",
    "negative_reviews=preprocessed_data(negative_reviews)\n",
    "\n",
    "positive_reviews,labels = split_label_review(positive_reviews)\n",
    "print(\"First few positive reviews:\",positive_reviews[:5])\n",
    "positive_reviews=preprocessed_data(positive_reviews)\n",
    "\n",
    "del positive_reviews, negative_reviews,labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
